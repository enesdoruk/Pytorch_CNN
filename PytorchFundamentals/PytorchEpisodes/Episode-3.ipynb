{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- supervised Learning\n",
    "- Unsupervised learning\n",
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsupervised Learning\n",
    "    - Clustering\n",
    "    - Dimensionality Reduction\n",
    "\n",
    "- Supervised Learning\n",
    "    - Classification\n",
    "    - Regression\n",
    "    - Segmentation\n",
    "    - Speech Recohnation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three popular holdout strategies that can be used to split the data into training and validation sets. There are as follows\n",
    "\n",
    "- Simple holdout validation\n",
    "- K-fold validation\n",
    "- Iterated k-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(path, '*/*.jpg'))\n",
    "no_of_images = len(files)\n",
    "shuffle = np.random.permutation(no_of_images)\n",
    "train = files[shuffle[:int(no_of_images*0.8)]]\n",
    "valid = files[shuffle[int(no_of_images*0.8):]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a fraction of the dataset for the test split, then divide the entire dataset into k folds where k can be any number, generally varying from two to ten. At any given ıteratıon we hold one block for validation and train the algoritm on the rest of the blockö the fınal score ıs generally the average of all the scores obtaıned across the k folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vectorization \n",
    "- Normalization\n",
    "- Missing values\n",
    "- Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes in various formats such as text sound images and video. The very first thing that needs to be done is to convert the data into pytorch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps ın traınıng the algorithms faster and helps in achieving more performance. Normalization is the process in which you represent data belogging to a particular feature in such a away that its mean is zero and standart deviation is one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- getting more data\n",
    "- reducing the size of the network\n",
    "- applying weight regularizer\n",
    "- applying dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting More data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your are able to get more data on which the algorithm can train that can help the algorithm to avoid overfitting by focusing on general patterns rather than on patterns specific to small data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization provides constrains on the network by penalizing the model when the weights of the model are larger\n",
    "\n",
    "L1 = The sum of obsolute values of weight coefficients are added to the cost. it is often referred to as the l1 norm of the weights\n",
    "\n",
    "L2 = The sum of squares of all weight coefficient are added to the cost. it is often referred to as l2 norm of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Architecture1(10,20,2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold of dropout values in the range 0.2 to 0.5, Dropouts are used to only during the training times and during the testing values are scaled down by the factor equal to the dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout(x, training = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when your model underfits is to acquire more data for the algorithm to train on. Another approach is to increase the comlexity of the model by increasing the number of layers or by increasing the number of weights or parameters used by the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding Dropout\n",
    "- Trying different architecture\n",
    "- Adding L1 or L2 regularization\n",
    "- Trying different learning rates\n",
    "- Adding more features or more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    validate(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    validate(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
